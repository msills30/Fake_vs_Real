{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pipeline class from the transformers module. \n",
    "from transformers import pipeline\n",
    "# Initialize the pipeline to generate questions and answers using the distilbert-base-cased-distilled-squad model. \n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Autotokenizer class from the transformers module. \n",
    "from transformers import AutoTokenizer\n",
    "# Create an instance of the Autotokenizer class using the t5-base model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SentenceTransformer class from the sentence_transformers library. \n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Use the all-MiniLM-L6-v2 model.\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Top Trump Surrogate BRUTALLY Stabs Him In The...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. conservative leader optimistic of common ...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump proposes U.S. tax overhaul, stirs concer...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Court Forces Ohio To Allow Millions Of Illega...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Democrats say Trump agrees to work on immigrat...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text label\n",
       "0   Top Trump Surrogate BRUTALLY Stabs Him In The...  Fake\n",
       "1  U.S. conservative leader optimistic of common ...  Real\n",
       "2  Trump proposes U.S. tax overhaul, stirs concer...  Real\n",
       "3   Court Forces Ohio To Allow Millions Of Illega...  Fake\n",
       "4  Democrats say Trump agrees to work on immigrat...  Real"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('C:/Users/m8rqu/Documents/AI Bootcamp/fake_and_real_news/fakerealnews.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Top Trump Surrogate BRUTALLY Stabs Him In The Back: ‘He’s Pathetic’ (VIDEO) It s looking as though Republican presidential candidate Donald Trump is losing support even from within his own ranks. You know things are getting bad when even your top surrogates start turning against you, which is exactly what just happened on Fox News when Newt Gingrich called Trump  pathetic. Gingrich knows that Trump needs to keep his focus on Hillary Clinton if he even remotely wants to have a chance at defeating her. However, Trump has hurt feelings because many Republicans don t support his sexual assault against women have turned against him, including House Speaker Paul Ryan (R-WI). So, that has made Trump lash out as his own party.Gingrich said on Fox News: Look, first of all, let me just say about Trump, who I admire and I ve tried to help as much as I can. There s a big Trump and a little Trump. The little Trump is frankly pathetic. I mean, he s mad over not getting a phone call? Trump s referring to the fact that Paul Ryan didn t call to congratulate him after the debate. Probably because he didn t win despite what Trump s ego tells him.Gingrich also added: Donald Trump has one opponent. Her name is Hillary Clinton. Her name is not Paul Ryan. It s not anybody else. Trump doesn t seem to realize that the person he should be mad at is himself because he truly is his own worst enemy. This will ultimately lead to his defeat and he will have no one to blame but himself.Watch here via Politico:Featured Photo by Joe Raedle/Getty Images'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = data['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['top',\n",
       " 'trump',\n",
       " 'sur',\n",
       " '##rogate',\n",
       " 'brutally',\n",
       " 'stab',\n",
       " '##s',\n",
       " 'him',\n",
       " 'in',\n",
       " 'the',\n",
       " 'back',\n",
       " ':',\n",
       " '‘',\n",
       " 'he',\n",
       " '’',\n",
       " 's',\n",
       " 'pathetic',\n",
       " '’',\n",
       " '(',\n",
       " 'video',\n",
       " ')',\n",
       " 'it',\n",
       " 's',\n",
       " 'looking',\n",
       " 'as',\n",
       " 'though',\n",
       " 'republican',\n",
       " 'presidential',\n",
       " 'candidate',\n",
       " 'donald',\n",
       " 'trump',\n",
       " 'is',\n",
       " 'losing',\n",
       " 'support',\n",
       " 'even',\n",
       " 'from',\n",
       " 'within',\n",
       " 'his',\n",
       " 'own',\n",
       " 'ranks',\n",
       " '.',\n",
       " 'you',\n",
       " 'know',\n",
       " 'things',\n",
       " 'are',\n",
       " 'getting',\n",
       " 'bad',\n",
       " 'when',\n",
       " 'even',\n",
       " 'your',\n",
       " 'top',\n",
       " 'sur',\n",
       " '##rogate',\n",
       " '##s',\n",
       " 'start',\n",
       " 'turning',\n",
       " 'against',\n",
       " 'you',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'exactly',\n",
       " 'what',\n",
       " 'just',\n",
       " 'happened',\n",
       " 'on',\n",
       " 'fox',\n",
       " 'news',\n",
       " 'when',\n",
       " 'newt',\n",
       " 'gin',\n",
       " '##gr',\n",
       " '##ich',\n",
       " 'called',\n",
       " 'trump',\n",
       " 'pathetic',\n",
       " '.',\n",
       " 'gin',\n",
       " '##gr',\n",
       " '##ich',\n",
       " 'knows',\n",
       " 'that',\n",
       " 'trump',\n",
       " 'needs',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'his',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'hillary',\n",
       " 'clinton',\n",
       " 'if',\n",
       " 'he',\n",
       " 'even',\n",
       " 'remotely',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'chance',\n",
       " 'at',\n",
       " 'defeating',\n",
       " 'her',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'trump',\n",
       " 'has',\n",
       " 'hurt',\n",
       " 'feelings',\n",
       " 'because',\n",
       " 'many',\n",
       " 'republicans',\n",
       " 'don',\n",
       " 't',\n",
       " 'support',\n",
       " 'his',\n",
       " 'sexual',\n",
       " 'assault',\n",
       " 'against',\n",
       " 'women',\n",
       " 'have',\n",
       " 'turned',\n",
       " 'against',\n",
       " 'him',\n",
       " ',',\n",
       " 'including',\n",
       " 'house',\n",
       " 'speaker',\n",
       " 'paul',\n",
       " 'ryan',\n",
       " '(',\n",
       " 'r',\n",
       " '-',\n",
       " 'wi',\n",
       " ')',\n",
       " '.',\n",
       " 'so',\n",
       " ',',\n",
       " 'that',\n",
       " 'has',\n",
       " 'made',\n",
       " 'trump',\n",
       " 'lash',\n",
       " 'out',\n",
       " 'as',\n",
       " 'his',\n",
       " 'own',\n",
       " 'party',\n",
       " '.',\n",
       " 'gin',\n",
       " '##gr',\n",
       " '##ich',\n",
       " 'said',\n",
       " 'on',\n",
       " 'fox',\n",
       " 'news',\n",
       " ':',\n",
       " 'look',\n",
       " ',',\n",
       " 'first',\n",
       " 'of',\n",
       " 'all',\n",
       " ',',\n",
       " 'let',\n",
       " 'me',\n",
       " 'just',\n",
       " 'say',\n",
       " 'about',\n",
       " 'trump',\n",
       " ',',\n",
       " 'who',\n",
       " 'i',\n",
       " 'admire',\n",
       " 'and',\n",
       " 'i',\n",
       " 've',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'help',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'i',\n",
       " 'can',\n",
       " '.',\n",
       " 'there',\n",
       " 's',\n",
       " 'a',\n",
       " 'big',\n",
       " 'trump',\n",
       " 'and',\n",
       " 'a',\n",
       " 'little',\n",
       " 'trump',\n",
       " '.',\n",
       " 'the',\n",
       " 'little',\n",
       " 'trump',\n",
       " 'is',\n",
       " 'frankly',\n",
       " 'pathetic',\n",
       " '.',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'he',\n",
       " 's',\n",
       " 'mad',\n",
       " 'over',\n",
       " 'not',\n",
       " 'getting',\n",
       " 'a',\n",
       " 'phone',\n",
       " 'call',\n",
       " '?',\n",
       " 'trump',\n",
       " 's',\n",
       " 'referring',\n",
       " 'to',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'paul',\n",
       " 'ryan',\n",
       " 'didn',\n",
       " 't',\n",
       " 'call',\n",
       " 'to',\n",
       " 'cong',\n",
       " '##rat',\n",
       " '##ulate',\n",
       " 'him',\n",
       " 'after',\n",
       " 'the',\n",
       " 'debate',\n",
       " '.',\n",
       " 'probably',\n",
       " 'because',\n",
       " 'he',\n",
       " 'didn',\n",
       " 't',\n",
       " 'win',\n",
       " 'despite',\n",
       " 'what',\n",
       " 'trump',\n",
       " 's',\n",
       " 'ego',\n",
       " 'tells',\n",
       " 'him',\n",
       " '.',\n",
       " 'gin',\n",
       " '##gr',\n",
       " '##ich',\n",
       " 'also',\n",
       " 'added',\n",
       " ':',\n",
       " 'donald',\n",
       " 'trump',\n",
       " 'has',\n",
       " 'one',\n",
       " 'opponent',\n",
       " '.',\n",
       " 'her',\n",
       " 'name',\n",
       " 'is',\n",
       " 'hillary',\n",
       " 'clinton',\n",
       " '.',\n",
       " 'her',\n",
       " 'name',\n",
       " 'is',\n",
       " 'not',\n",
       " 'paul',\n",
       " 'ryan',\n",
       " '.',\n",
       " 'it',\n",
       " 's',\n",
       " 'not',\n",
       " 'anybody',\n",
       " 'else',\n",
       " '.',\n",
       " 'trump',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'realize',\n",
       " 'that',\n",
       " 'the',\n",
       " 'person',\n",
       " 'he',\n",
       " 'should',\n",
       " 'be',\n",
       " 'mad',\n",
       " 'at',\n",
       " 'is',\n",
       " 'himself',\n",
       " 'because',\n",
       " 'he',\n",
       " 'truly',\n",
       " 'is',\n",
       " 'his',\n",
       " 'own',\n",
       " 'worst',\n",
       " 'enemy',\n",
       " '.',\n",
       " 'this',\n",
       " 'will',\n",
       " 'ultimately',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'his',\n",
       " 'defeat',\n",
       " 'and',\n",
       " 'he',\n",
       " 'will',\n",
       " 'have',\n",
       " 'no',\n",
       " 'one',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'but',\n",
       " 'himself',\n",
       " '.',\n",
       " 'watch',\n",
       " 'here',\n",
       " 'via',\n",
       " 'pol',\n",
       " '##itic',\n",
       " '##o',\n",
       " ':',\n",
       " 'featured',\n",
       " 'photo',\n",
       " 'by',\n",
       " 'joe',\n",
       " 'rae',\n",
       " '##dle',\n",
       " '/',\n",
       " 'get',\n",
       " '##ty',\n",
       " 'images']"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the sentence with model. \n",
    "tokens = model.tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2327, 8398, 7505, 21799, 23197, 17079, 2015, 2032, 1999, 1996, 2067, 1024, 1520, 2002, 1521, 1055, 17203, 1521, 1006, 2678, 1007, 2009, 1055, 2559, 2004, 2295, 3951, 4883, 4018, 6221, 8398, 2003, 3974, 2490, 2130, 2013, 2306, 2010, 2219, 6938, 1012, 2017, 2113, 2477, 2024, 2893, 2919, 2043, 2130, 2115, 2327, 7505, 21799, 2015, 2707, 3810, 2114, 2017, 1010, 2029, 2003, 3599, 2054, 2074, 3047, 2006, 4419, 2739, 2043, 25597, 18353, 16523, 7033, 2170, 8398, 17203, 1012, 18353, 16523, 7033, 4282, 2008, 8398, 3791, 2000, 2562, 2010, 3579, 2006, 18520, 7207, 2065, 2002, 2130, 19512, 4122, 2000, 2031, 1037, 3382, 2012, 6324, 2014, 1012, 2174, 1010, 8398, 2038, 3480, 5346, 2138, 2116, 10643, 2123, 1056, 2490, 2010, 4424, 6101, 2114, 2308, 2031, 2357, 2114, 2032, 1010, 2164, 2160, 5882, 2703, 4575, 1006, 1054, 1011, 15536, 1007, 1012, 2061, 1010, 2008, 2038, 2081, 8398, 25210, 2041, 2004, 2010, 2219, 2283, 1012, 18353, 16523, 7033, 2056, 2006, 4419, 2739, 1024, 2298, 1010, 2034, 1997, 2035, 1010, 2292, 2033, 2074, 2360, 2055, 8398, 1010, 2040, 1045, 19837, 1998, 1045, 2310, 2699, 2000, 2393, 2004, 2172, 2004, 1045, 2064, 1012, 2045, 1055, 1037, 2502, 8398, 1998, 1037, 2210, 8398, 1012, 1996, 2210, 8398, 2003, 19597, 17203, 1012, 1045, 2812, 1010, 2002, 1055, 5506, 2058, 2025, 2893, 1037, 3042, 2655, 1029, 8398, 1055, 7727, 2000, 1996, 2755, 2008, 2703, 4575, 2134, 1056, 2655, 2000, 26478, 8609, 9869, 2032, 2044, 1996, 5981, 1012, 2763, 2138, 2002, 2134, 1056, 2663, 2750, 2054, 8398, 1055, 13059, 4136, 2032, 1012, 18353, 16523, 7033, 2036, 2794, 1024, 6221, 8398, 2038, 2028, 7116, 1012, 2014, 2171, 2003, 18520, 7207, 1012, 2014, 2171, 2003, 2025, 2703, 4575, 1012, 2009, 1055, 2025, 10334, 2842, 1012, 8398, 2987, 1056, 4025, 2000, 5382, 2008, 1996, 2711, 2002, 2323, 2022, 5506, 2012, 2003, 2370, 2138, 2002, 5621, 2003, 2010, 2219, 5409, 4099, 1012, 2023, 2097, 4821, 2599, 2000, 2010, 4154, 1998, 2002, 2097, 2031, 2053, 2028, 2000, 7499, 2021, 2370, 1012, 3422, 2182, 3081, 14955, 18291, 2080, 1024, 2956, 6302, 2011, 3533, 14786, 10362, 1013, 2131, 3723, 4871]\n"
     ]
    }
   ],
   "source": [
    "# Convert the tokens to IDs.\n",
    "ids = model.tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1249 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the text\n",
    "tokenized_texts = []\n",
    "for text in data['Text']:\n",
    "    tokens = model.tokenizer.tokenize(text)\n",
    "    token_ids = model.tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokenized_texts.append(token_ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Fake\n",
       "1       Real\n",
       "2       Real\n",
       "3       Fake\n",
       "4       Real\n",
       "        ... \n",
       "9895    Fake\n",
       "9896    Real\n",
       "9897    Real\n",
       "9898    Fake\n",
       "9899    Fake\n",
       "Name: label, Length: 9900, dtype: object"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data['label']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2327, 8398, 7505, 21799, 23197, 17079, 2015, ...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1057, 1012, 1055, 1012, 4603, 3003, 21931, 19...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[8398, 17146, 1057, 1012, 1055, 1012, 4171, 18...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2457, 2749, 4058, 2000, 3499, 8817, 1997, 178...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[8037, 2360, 8398, 10217, 2000, 2147, 2006, 75...</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Tokenized_Text Label\n",
       "0  [2327, 8398, 7505, 21799, 23197, 17079, 2015, ...  Fake\n",
       "1  [1057, 1012, 1055, 1012, 4603, 3003, 21931, 19...  Real\n",
       "2  [8398, 17146, 1057, 1012, 1055, 1012, 4171, 18...  Real\n",
       "3  [2457, 2749, 4058, 2000, 3499, 8817, 1997, 178...  Fake\n",
       "4  [8037, 2360, 8398, 10217, 2000, 2147, 2006, 75...  Real"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = pd.DataFrame({'Tokenized_Text': tokenized_texts, 'Label': labels})\n",
    "tokenized_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6329</th>\n",
       "      <th>6330</th>\n",
       "      <th>6331</th>\n",
       "      <th>6332</th>\n",
       "      <th>6333</th>\n",
       "      <th>6334</th>\n",
       "      <th>6335</th>\n",
       "      <th>6336</th>\n",
       "      <th>6337</th>\n",
       "      <th>6338</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fake</th>\n",
       "      <td>2327</td>\n",
       "      <td>8398</td>\n",
       "      <td>7505</td>\n",
       "      <td>21799</td>\n",
       "      <td>23197</td>\n",
       "      <td>17079</td>\n",
       "      <td>2015</td>\n",
       "      <td>2032</td>\n",
       "      <td>1999</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <td>1057</td>\n",
       "      <td>1012</td>\n",
       "      <td>1055</td>\n",
       "      <td>1012</td>\n",
       "      <td>4603</td>\n",
       "      <td>3003</td>\n",
       "      <td>21931</td>\n",
       "      <td>1997</td>\n",
       "      <td>2691</td>\n",
       "      <td>2598</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <td>8398</td>\n",
       "      <td>17146</td>\n",
       "      <td>1057</td>\n",
       "      <td>1012</td>\n",
       "      <td>1055</td>\n",
       "      <td>1012</td>\n",
       "      <td>4171</td>\n",
       "      <td>18181</td>\n",
       "      <td>1010</td>\n",
       "      <td>16130</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fake</th>\n",
       "      <td>2457</td>\n",
       "      <td>2749</td>\n",
       "      <td>4058</td>\n",
       "      <td>2000</td>\n",
       "      <td>3499</td>\n",
       "      <td>8817</td>\n",
       "      <td>1997</td>\n",
       "      <td>17800</td>\n",
       "      <td>24694</td>\n",
       "      <td>2094</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <td>8037</td>\n",
       "      <td>2360</td>\n",
       "      <td>8398</td>\n",
       "      <td>10217</td>\n",
       "      <td>2000</td>\n",
       "      <td>2147</td>\n",
       "      <td>2006</td>\n",
       "      <td>7521</td>\n",
       "      <td>3021</td>\n",
       "      <td>1010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fake</th>\n",
       "      <td>15536</td>\n",
       "      <td>3211</td>\n",
       "      <td>19738</td>\n",
       "      <td>5705</td>\n",
       "      <td>14456</td>\n",
       "      <td>2000</td>\n",
       "      <td>29082</td>\n",
       "      <td>2039</td>\n",
       "      <td>24256</td>\n",
       "      <td>2007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <td>8398</td>\n",
       "      <td>23363</td>\n",
       "      <td>2015</td>\n",
       "      <td>3951</td>\n",
       "      <td>10153</td>\n",
       "      <td>2006</td>\n",
       "      <td>7349</td>\n",
       "      <td>2708</td>\n",
       "      <td>5347</td>\n",
       "      <td>2899</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <td>8398</td>\n",
       "      <td>9559</td>\n",
       "      <td>2360</td>\n",
       "      <td>3648</td>\n",
       "      <td>14087</td>\n",
       "      <td>7360</td>\n",
       "      <td>2005</td>\n",
       "      <td>27652</td>\n",
       "      <td>9870</td>\n",
       "      <td>2047</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fake</th>\n",
       "      <td>3422</td>\n",
       "      <td>1024</td>\n",
       "      <td>2157</td>\n",
       "      <td>1011</td>\n",
       "      <td>3358</td>\n",
       "      <td>9220</td>\n",
       "      <td>23123</td>\n",
       "      <td>6495</td>\n",
       "      <td>8398</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fake</th>\n",
       "      <td>5977</td>\n",
       "      <td>17688</td>\n",
       "      <td>2099</td>\n",
       "      <td>26316</td>\n",
       "      <td>2135</td>\n",
       "      <td>11180</td>\n",
       "      <td>2004</td>\n",
       "      <td>21868</td>\n",
       "      <td>2232</td>\n",
       "      <td>1008</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9900 rows × 6339 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1      2      3      4      5      6      7      8      9     \\\n",
       "label                                                                         \n",
       "Fake    2327   8398   7505  21799  23197  17079   2015   2032   1999   1996   \n",
       "Real    1057   1012   1055   1012   4603   3003  21931   1997   2691   2598   \n",
       "Real    8398  17146   1057   1012   1055   1012   4171  18181   1010  16130   \n",
       "Fake    2457   2749   4058   2000   3499   8817   1997  17800  24694   2094   \n",
       "Real    8037   2360   8398  10217   2000   2147   2006   7521   3021   1010   \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "Fake   15536   3211  19738   5705  14456   2000  29082   2039  24256   2007   \n",
       "Real    8398  23363   2015   3951  10153   2006   7349   2708   5347   2899   \n",
       "Real    8398   9559   2360   3648  14087   7360   2005  27652   9870   2047   \n",
       "Fake    3422   1024   2157   1011   3358   9220  23123   6495   8398   2005   \n",
       "Fake    5977  17688   2099  26316   2135  11180   2004  21868   2232   1008   \n",
       "\n",
       "       ...  6329  6330  6331  6332  6333  6334  6335  6336  6337  6338  \n",
       "label  ...                                                              \n",
       "Fake   ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "Real   ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "Real   ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "Fake   ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "Real   ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "Fake   ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "Real   ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "Real   ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "Fake   ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "Fake   ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[9900 rows x 6339 columns]"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = pd.DataFrame(tokenized_texts,labels)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenized_data.dropna(axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fake</th>\n",
       "      <td>2327</td>\n",
       "      <td>8398</td>\n",
       "      <td>7505</td>\n",
       "      <td>21799</td>\n",
       "      <td>23197</td>\n",
       "      <td>17079</td>\n",
       "      <td>2015</td>\n",
       "      <td>2032</td>\n",
       "      <td>1999</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>2013</td>\n",
       "      <td>2306</td>\n",
       "      <td>2010</td>\n",
       "      <td>2219</td>\n",
       "      <td>6938</td>\n",
       "      <td>1012</td>\n",
       "      <td>2017</td>\n",
       "      <td>2113</td>\n",
       "      <td>2477</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <td>1057</td>\n",
       "      <td>1012</td>\n",
       "      <td>1055</td>\n",
       "      <td>1012</td>\n",
       "      <td>4603</td>\n",
       "      <td>3003</td>\n",
       "      <td>21931</td>\n",
       "      <td>1997</td>\n",
       "      <td>2691</td>\n",
       "      <td>2598</td>\n",
       "      <td>...</td>\n",
       "      <td>2663</td>\n",
       "      <td>2490</td>\n",
       "      <td>2005</td>\n",
       "      <td>9871</td>\n",
       "      <td>6094</td>\n",
       "      <td>2006</td>\n",
       "      <td>9432</td>\n",
       "      <td>1010</td>\n",
       "      <td>2044</td>\n",
       "      <td>2343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <td>8398</td>\n",
       "      <td>17146</td>\n",
       "      <td>1057</td>\n",
       "      <td>1012</td>\n",
       "      <td>1055</td>\n",
       "      <td>1012</td>\n",
       "      <td>4171</td>\n",
       "      <td>18181</td>\n",
       "      <td>1010</td>\n",
       "      <td>16130</td>\n",
       "      <td>...</td>\n",
       "      <td>5109</td>\n",
       "      <td>1010</td>\n",
       "      <td>4214</td>\n",
       "      <td>2005</td>\n",
       "      <td>4171</td>\n",
       "      <td>7659</td>\n",
       "      <td>2005</td>\n",
       "      <td>2087</td>\n",
       "      <td>4841</td>\n",
       "      <td>1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fake</th>\n",
       "      <td>2457</td>\n",
       "      <td>2749</td>\n",
       "      <td>4058</td>\n",
       "      <td>2000</td>\n",
       "      <td>3499</td>\n",
       "      <td>8817</td>\n",
       "      <td>1997</td>\n",
       "      <td>17800</td>\n",
       "      <td>24694</td>\n",
       "      <td>2094</td>\n",
       "      <td>...</td>\n",
       "      <td>1996</td>\n",
       "      <td>2111</td>\n",
       "      <td>1010</td>\n",
       "      <td>2025</td>\n",
       "      <td>2032</td>\n",
       "      <td>1012</td>\n",
       "      <td>2202</td>\n",
       "      <td>1037</td>\n",
       "      <td>2298</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <td>8037</td>\n",
       "      <td>2360</td>\n",
       "      <td>8398</td>\n",
       "      <td>10217</td>\n",
       "      <td>2000</td>\n",
       "      <td>2147</td>\n",
       "      <td>2006</td>\n",
       "      <td>7521</td>\n",
       "      <td>3021</td>\n",
       "      <td>1010</td>\n",
       "      <td>...</td>\n",
       "      <td>2362</td>\n",
       "      <td>2006</td>\n",
       "      <td>6094</td>\n",
       "      <td>2000</td>\n",
       "      <td>4047</td>\n",
       "      <td>1523</td>\n",
       "      <td>24726</td>\n",
       "      <td>2015</td>\n",
       "      <td>1010</td>\n",
       "      <td>1524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fake</th>\n",
       "      <td>15536</td>\n",
       "      <td>3211</td>\n",
       "      <td>19738</td>\n",
       "      <td>5705</td>\n",
       "      <td>14456</td>\n",
       "      <td>2000</td>\n",
       "      <td>29082</td>\n",
       "      <td>2039</td>\n",
       "      <td>24256</td>\n",
       "      <td>2007</td>\n",
       "      <td>...</td>\n",
       "      <td>3422</td>\n",
       "      <td>16168</td>\n",
       "      <td>2122</td>\n",
       "      <td>2420</td>\n",
       "      <td>1010</td>\n",
       "      <td>2404</td>\n",
       "      <td>2039</td>\n",
       "      <td>1037</td>\n",
       "      <td>8554</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <td>8398</td>\n",
       "      <td>23363</td>\n",
       "      <td>2015</td>\n",
       "      <td>3951</td>\n",
       "      <td>10153</td>\n",
       "      <td>2006</td>\n",
       "      <td>7349</td>\n",
       "      <td>2708</td>\n",
       "      <td>5347</td>\n",
       "      <td>2899</td>\n",
       "      <td>...</td>\n",
       "      <td>2040</td>\n",
       "      <td>2002</td>\n",
       "      <td>2323</td>\n",
       "      <td>11112</td>\n",
       "      <td>2000</td>\n",
       "      <td>2022</td>\n",
       "      <td>1996</td>\n",
       "      <td>2279</td>\n",
       "      <td>3003</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real</th>\n",
       "      <td>8398</td>\n",
       "      <td>9559</td>\n",
       "      <td>2360</td>\n",
       "      <td>3648</td>\n",
       "      <td>14087</td>\n",
       "      <td>7360</td>\n",
       "      <td>2005</td>\n",
       "      <td>27652</td>\n",
       "      <td>9870</td>\n",
       "      <td>2047</td>\n",
       "      <td>...</td>\n",
       "      <td>1996</td>\n",
       "      <td>1057</td>\n",
       "      <td>1012</td>\n",
       "      <td>1055</td>\n",
       "      <td>1012</td>\n",
       "      <td>4552</td>\n",
       "      <td>2016</td>\n",
       "      <td>2018</td>\n",
       "      <td>2053</td>\n",
       "      <td>7360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fake</th>\n",
       "      <td>3422</td>\n",
       "      <td>1024</td>\n",
       "      <td>2157</td>\n",
       "      <td>1011</td>\n",
       "      <td>3358</td>\n",
       "      <td>9220</td>\n",
       "      <td>23123</td>\n",
       "      <td>6495</td>\n",
       "      <td>8398</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>3068</td>\n",
       "      <td>2035</td>\n",
       "      <td>2011</td>\n",
       "      <td>2370</td>\n",
       "      <td>1029</td>\n",
       "      <td>3398</td>\n",
       "      <td>1010</td>\n",
       "      <td>2033</td>\n",
       "      <td>4445</td>\n",
       "      <td>1012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fake</th>\n",
       "      <td>5977</td>\n",
       "      <td>17688</td>\n",
       "      <td>2099</td>\n",
       "      <td>26316</td>\n",
       "      <td>2135</td>\n",
       "      <td>11180</td>\n",
       "      <td>2004</td>\n",
       "      <td>21868</td>\n",
       "      <td>2232</td>\n",
       "      <td>1008</td>\n",
       "      <td>...</td>\n",
       "      <td>2138</td>\n",
       "      <td>2057</td>\n",
       "      <td>2293</td>\n",
       "      <td>2000</td>\n",
       "      <td>5223</td>\n",
       "      <td>2032</td>\n",
       "      <td>1012</td>\n",
       "      <td>2010</td>\n",
       "      <td>14002</td>\n",
       "      <td>2105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9900 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3      4      5      6      7      8      9   \\\n",
       "label                                                                         \n",
       "Fake    2327   8398   7505  21799  23197  17079   2015   2032   1999   1996   \n",
       "Real    1057   1012   1055   1012   4603   3003  21931   1997   2691   2598   \n",
       "Real    8398  17146   1057   1012   1055   1012   4171  18181   1010  16130   \n",
       "Fake    2457   2749   4058   2000   3499   8817   1997  17800  24694   2094   \n",
       "Real    8037   2360   8398  10217   2000   2147   2006   7521   3021   1010   \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "Fake   15536   3211  19738   5705  14456   2000  29082   2039  24256   2007   \n",
       "Real    8398  23363   2015   3951  10153   2006   7349   2708   5347   2899   \n",
       "Real    8398   9559   2360   3648  14087   7360   2005  27652   9870   2047   \n",
       "Fake    3422   1024   2157   1011   3358   9220  23123   6495   8398   2005   \n",
       "Fake    5977  17688   2099  26316   2135  11180   2004  21868   2232   1008   \n",
       "\n",
       "       ...    35     36    37     38    39    40     41    42     43    44  \n",
       "label  ...                                                                  \n",
       "Fake   ...  2013   2306  2010   2219  6938  1012   2017  2113   2477  2024  \n",
       "Real   ...  2663   2490  2005   9871  6094  2006   9432  1010   2044  2343  \n",
       "Real   ...  5109   1010  4214   2005  4171  7659   2005  2087   4841  1010  \n",
       "Fake   ...  1996   2111  1010   2025  2032  1012   2202  1037   2298  2012  \n",
       "Real   ...  2362   2006  6094   2000  4047  1523  24726  2015   1010  1524  \n",
       "...    ...   ...    ...   ...    ...   ...   ...    ...   ...    ...   ...  \n",
       "Fake   ...  3422  16168  2122   2420  1010  2404   2039  1037   8554  2006  \n",
       "Real   ...  2040   2002  2323  11112  2000  2022   1996  2279   3003  1997  \n",
       "Real   ...  1996   1057  1012   1055  1012  4552   2016  2018   2053  7360  \n",
       "Fake   ...  3068   2035  2011   2370  1029  3398   1010  2033   4445  1012  \n",
       "Fake   ...  2138   2057  2293   2000  5223  2032   1012  2010  14002  2105  \n",
       "\n",
       "[9900 rows x 45 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Fake', 'Real', 'Real', 'Fake', 'Real', 'Real', 'Real', 'Fake', 'Real',\n",
       "       'Real',\n",
       "       ...\n",
       "       'Real', 'Fake', 'Real', 'Fake', 'Real', 'Fake', 'Real', 'Real', 'Fake',\n",
       "       'Fake'],\n",
       "      dtype='object', name='label', length=9900)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for tokenized texts\n",
    "tokenized_data = pd.DataFrame(tokenized_texts)\n",
    "\n",
    "# Create a DataFrame for labels\n",
    "label_data = pd.DataFrame({'Label': labels})\n",
    "\n",
    "# Reset the index of the label DataFrame\n",
    "label_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate the two DataFrames along the columns axis\n",
    "final_data = pd.concat([tokenized_data, label_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6330</th>\n",
       "      <th>6331</th>\n",
       "      <th>6332</th>\n",
       "      <th>6333</th>\n",
       "      <th>6334</th>\n",
       "      <th>6335</th>\n",
       "      <th>6336</th>\n",
       "      <th>6337</th>\n",
       "      <th>6338</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2327</td>\n",
       "      <td>8398</td>\n",
       "      <td>7505</td>\n",
       "      <td>21799</td>\n",
       "      <td>23197</td>\n",
       "      <td>17079</td>\n",
       "      <td>2015</td>\n",
       "      <td>2032</td>\n",
       "      <td>1999</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1057</td>\n",
       "      <td>1012</td>\n",
       "      <td>1055</td>\n",
       "      <td>1012</td>\n",
       "      <td>4603</td>\n",
       "      <td>3003</td>\n",
       "      <td>21931</td>\n",
       "      <td>1997</td>\n",
       "      <td>2691</td>\n",
       "      <td>2598</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8398</td>\n",
       "      <td>17146</td>\n",
       "      <td>1057</td>\n",
       "      <td>1012</td>\n",
       "      <td>1055</td>\n",
       "      <td>1012</td>\n",
       "      <td>4171</td>\n",
       "      <td>18181</td>\n",
       "      <td>1010</td>\n",
       "      <td>16130</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2457</td>\n",
       "      <td>2749</td>\n",
       "      <td>4058</td>\n",
       "      <td>2000</td>\n",
       "      <td>3499</td>\n",
       "      <td>8817</td>\n",
       "      <td>1997</td>\n",
       "      <td>17800</td>\n",
       "      <td>24694</td>\n",
       "      <td>2094</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8037</td>\n",
       "      <td>2360</td>\n",
       "      <td>8398</td>\n",
       "      <td>10217</td>\n",
       "      <td>2000</td>\n",
       "      <td>2147</td>\n",
       "      <td>2006</td>\n",
       "      <td>7521</td>\n",
       "      <td>3021</td>\n",
       "      <td>1010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>15536</td>\n",
       "      <td>3211</td>\n",
       "      <td>19738</td>\n",
       "      <td>5705</td>\n",
       "      <td>14456</td>\n",
       "      <td>2000</td>\n",
       "      <td>29082</td>\n",
       "      <td>2039</td>\n",
       "      <td>24256</td>\n",
       "      <td>2007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>8398</td>\n",
       "      <td>23363</td>\n",
       "      <td>2015</td>\n",
       "      <td>3951</td>\n",
       "      <td>10153</td>\n",
       "      <td>2006</td>\n",
       "      <td>7349</td>\n",
       "      <td>2708</td>\n",
       "      <td>5347</td>\n",
       "      <td>2899</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>8398</td>\n",
       "      <td>9559</td>\n",
       "      <td>2360</td>\n",
       "      <td>3648</td>\n",
       "      <td>14087</td>\n",
       "      <td>7360</td>\n",
       "      <td>2005</td>\n",
       "      <td>27652</td>\n",
       "      <td>9870</td>\n",
       "      <td>2047</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>3422</td>\n",
       "      <td>1024</td>\n",
       "      <td>2157</td>\n",
       "      <td>1011</td>\n",
       "      <td>3358</td>\n",
       "      <td>9220</td>\n",
       "      <td>23123</td>\n",
       "      <td>6495</td>\n",
       "      <td>8398</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>5977</td>\n",
       "      <td>17688</td>\n",
       "      <td>2099</td>\n",
       "      <td>26316</td>\n",
       "      <td>2135</td>\n",
       "      <td>11180</td>\n",
       "      <td>2004</td>\n",
       "      <td>21868</td>\n",
       "      <td>2232</td>\n",
       "      <td>1008</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9900 rows × 6340 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3      4      5      6      7      8      9  \\\n",
       "0      2327   8398   7505  21799  23197  17079   2015   2032   1999   1996   \n",
       "1      1057   1012   1055   1012   4603   3003  21931   1997   2691   2598   \n",
       "2      8398  17146   1057   1012   1055   1012   4171  18181   1010  16130   \n",
       "3      2457   2749   4058   2000   3499   8817   1997  17800  24694   2094   \n",
       "4      8037   2360   8398  10217   2000   2147   2006   7521   3021   1010   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "9895  15536   3211  19738   5705  14456   2000  29082   2039  24256   2007   \n",
       "9896   8398  23363   2015   3951  10153   2006   7349   2708   5347   2899   \n",
       "9897   8398   9559   2360   3648  14087   7360   2005  27652   9870   2047   \n",
       "9898   3422   1024   2157   1011   3358   9220  23123   6495   8398   2005   \n",
       "9899   5977  17688   2099  26316   2135  11180   2004  21868   2232   1008   \n",
       "\n",
       "      ...  6330  6331  6332  6333  6334  6335  6336  6337  6338  Label  \n",
       "0     ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   Fake  \n",
       "1     ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   Real  \n",
       "2     ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   Real  \n",
       "3     ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   Fake  \n",
       "4     ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   Real  \n",
       "...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...    ...  \n",
       "9895  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   Fake  \n",
       "9896  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   Real  \n",
       "9897  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   Real  \n",
       "9898  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   Fake  \n",
       "9899  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   Fake  \n",
       "\n",
       "[9900 rows x 6340 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "final_data['New_Label'] = np.where(final_data['Label'] == 'Fake', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6330</th>\n",
       "      <th>6331</th>\n",
       "      <th>6332</th>\n",
       "      <th>6333</th>\n",
       "      <th>6334</th>\n",
       "      <th>6335</th>\n",
       "      <th>6336</th>\n",
       "      <th>6337</th>\n",
       "      <th>6338</th>\n",
       "      <th>New_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2327</td>\n",
       "      <td>8398</td>\n",
       "      <td>7505</td>\n",
       "      <td>21799</td>\n",
       "      <td>23197</td>\n",
       "      <td>17079</td>\n",
       "      <td>2015</td>\n",
       "      <td>2032</td>\n",
       "      <td>1999</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1057</td>\n",
       "      <td>1012</td>\n",
       "      <td>1055</td>\n",
       "      <td>1012</td>\n",
       "      <td>4603</td>\n",
       "      <td>3003</td>\n",
       "      <td>21931</td>\n",
       "      <td>1997</td>\n",
       "      <td>2691</td>\n",
       "      <td>2598</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8398</td>\n",
       "      <td>17146</td>\n",
       "      <td>1057</td>\n",
       "      <td>1012</td>\n",
       "      <td>1055</td>\n",
       "      <td>1012</td>\n",
       "      <td>4171</td>\n",
       "      <td>18181</td>\n",
       "      <td>1010</td>\n",
       "      <td>16130</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2457</td>\n",
       "      <td>2749</td>\n",
       "      <td>4058</td>\n",
       "      <td>2000</td>\n",
       "      <td>3499</td>\n",
       "      <td>8817</td>\n",
       "      <td>1997</td>\n",
       "      <td>17800</td>\n",
       "      <td>24694</td>\n",
       "      <td>2094</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8037</td>\n",
       "      <td>2360</td>\n",
       "      <td>8398</td>\n",
       "      <td>10217</td>\n",
       "      <td>2000</td>\n",
       "      <td>2147</td>\n",
       "      <td>2006</td>\n",
       "      <td>7521</td>\n",
       "      <td>3021</td>\n",
       "      <td>1010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>15536</td>\n",
       "      <td>3211</td>\n",
       "      <td>19738</td>\n",
       "      <td>5705</td>\n",
       "      <td>14456</td>\n",
       "      <td>2000</td>\n",
       "      <td>29082</td>\n",
       "      <td>2039</td>\n",
       "      <td>24256</td>\n",
       "      <td>2007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>8398</td>\n",
       "      <td>23363</td>\n",
       "      <td>2015</td>\n",
       "      <td>3951</td>\n",
       "      <td>10153</td>\n",
       "      <td>2006</td>\n",
       "      <td>7349</td>\n",
       "      <td>2708</td>\n",
       "      <td>5347</td>\n",
       "      <td>2899</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>8398</td>\n",
       "      <td>9559</td>\n",
       "      <td>2360</td>\n",
       "      <td>3648</td>\n",
       "      <td>14087</td>\n",
       "      <td>7360</td>\n",
       "      <td>2005</td>\n",
       "      <td>27652</td>\n",
       "      <td>9870</td>\n",
       "      <td>2047</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>3422</td>\n",
       "      <td>1024</td>\n",
       "      <td>2157</td>\n",
       "      <td>1011</td>\n",
       "      <td>3358</td>\n",
       "      <td>9220</td>\n",
       "      <td>23123</td>\n",
       "      <td>6495</td>\n",
       "      <td>8398</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>5977</td>\n",
       "      <td>17688</td>\n",
       "      <td>2099</td>\n",
       "      <td>26316</td>\n",
       "      <td>2135</td>\n",
       "      <td>11180</td>\n",
       "      <td>2004</td>\n",
       "      <td>21868</td>\n",
       "      <td>2232</td>\n",
       "      <td>1008</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9900 rows × 6340 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3      4      5      6      7      8      9  \\\n",
       "0      2327   8398   7505  21799  23197  17079   2015   2032   1999   1996   \n",
       "1      1057   1012   1055   1012   4603   3003  21931   1997   2691   2598   \n",
       "2      8398  17146   1057   1012   1055   1012   4171  18181   1010  16130   \n",
       "3      2457   2749   4058   2000   3499   8817   1997  17800  24694   2094   \n",
       "4      8037   2360   8398  10217   2000   2147   2006   7521   3021   1010   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "9895  15536   3211  19738   5705  14456   2000  29082   2039  24256   2007   \n",
       "9896   8398  23363   2015   3951  10153   2006   7349   2708   5347   2899   \n",
       "9897   8398   9559   2360   3648  14087   7360   2005  27652   9870   2047   \n",
       "9898   3422   1024   2157   1011   3358   9220  23123   6495   8398   2005   \n",
       "9899   5977  17688   2099  26316   2135  11180   2004  21868   2232   1008   \n",
       "\n",
       "      ...  6330  6331  6332  6333  6334  6335  6336  6337  6338  New_Label  \n",
       "0     ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN          0  \n",
       "1     ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN          1  \n",
       "2     ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN          1  \n",
       "3     ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN          0  \n",
       "4     ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN          1  \n",
       "...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...        ...  \n",
       "9895  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN          0  \n",
       "9896  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN          1  \n",
       "9897  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN          1  \n",
       "9898  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN          0  \n",
       "9899  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN          0  \n",
       "\n",
       "[9900 rows x 6340 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.drop(['Label'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6330</th>\n",
       "      <th>6331</th>\n",
       "      <th>6332</th>\n",
       "      <th>6333</th>\n",
       "      <th>6334</th>\n",
       "      <th>6335</th>\n",
       "      <th>6336</th>\n",
       "      <th>6337</th>\n",
       "      <th>6338</th>\n",
       "      <th>New_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2327</td>\n",
       "      <td>8398</td>\n",
       "      <td>7505</td>\n",
       "      <td>21799</td>\n",
       "      <td>23197</td>\n",
       "      <td>17079</td>\n",
       "      <td>2015</td>\n",
       "      <td>2032</td>\n",
       "      <td>1999</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1057</td>\n",
       "      <td>1012</td>\n",
       "      <td>1055</td>\n",
       "      <td>1012</td>\n",
       "      <td>4603</td>\n",
       "      <td>3003</td>\n",
       "      <td>21931</td>\n",
       "      <td>1997</td>\n",
       "      <td>2691</td>\n",
       "      <td>2598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8398</td>\n",
       "      <td>17146</td>\n",
       "      <td>1057</td>\n",
       "      <td>1012</td>\n",
       "      <td>1055</td>\n",
       "      <td>1012</td>\n",
       "      <td>4171</td>\n",
       "      <td>18181</td>\n",
       "      <td>1010</td>\n",
       "      <td>16130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2457</td>\n",
       "      <td>2749</td>\n",
       "      <td>4058</td>\n",
       "      <td>2000</td>\n",
       "      <td>3499</td>\n",
       "      <td>8817</td>\n",
       "      <td>1997</td>\n",
       "      <td>17800</td>\n",
       "      <td>24694</td>\n",
       "      <td>2094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8037</td>\n",
       "      <td>2360</td>\n",
       "      <td>8398</td>\n",
       "      <td>10217</td>\n",
       "      <td>2000</td>\n",
       "      <td>2147</td>\n",
       "      <td>2006</td>\n",
       "      <td>7521</td>\n",
       "      <td>3021</td>\n",
       "      <td>1010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9895</th>\n",
       "      <td>15536</td>\n",
       "      <td>3211</td>\n",
       "      <td>19738</td>\n",
       "      <td>5705</td>\n",
       "      <td>14456</td>\n",
       "      <td>2000</td>\n",
       "      <td>29082</td>\n",
       "      <td>2039</td>\n",
       "      <td>24256</td>\n",
       "      <td>2007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9896</th>\n",
       "      <td>8398</td>\n",
       "      <td>23363</td>\n",
       "      <td>2015</td>\n",
       "      <td>3951</td>\n",
       "      <td>10153</td>\n",
       "      <td>2006</td>\n",
       "      <td>7349</td>\n",
       "      <td>2708</td>\n",
       "      <td>5347</td>\n",
       "      <td>2899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>8398</td>\n",
       "      <td>9559</td>\n",
       "      <td>2360</td>\n",
       "      <td>3648</td>\n",
       "      <td>14087</td>\n",
       "      <td>7360</td>\n",
       "      <td>2005</td>\n",
       "      <td>27652</td>\n",
       "      <td>9870</td>\n",
       "      <td>2047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9898</th>\n",
       "      <td>3422</td>\n",
       "      <td>1024</td>\n",
       "      <td>2157</td>\n",
       "      <td>1011</td>\n",
       "      <td>3358</td>\n",
       "      <td>9220</td>\n",
       "      <td>23123</td>\n",
       "      <td>6495</td>\n",
       "      <td>8398</td>\n",
       "      <td>2005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9899</th>\n",
       "      <td>5977</td>\n",
       "      <td>17688</td>\n",
       "      <td>2099</td>\n",
       "      <td>26316</td>\n",
       "      <td>2135</td>\n",
       "      <td>11180</td>\n",
       "      <td>2004</td>\n",
       "      <td>21868</td>\n",
       "      <td>2232</td>\n",
       "      <td>1008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9900 rows × 6340 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3      4      5      6      7      8      9  \\\n",
       "0      2327   8398   7505  21799  23197  17079   2015   2032   1999   1996   \n",
       "1      1057   1012   1055   1012   4603   3003  21931   1997   2691   2598   \n",
       "2      8398  17146   1057   1012   1055   1012   4171  18181   1010  16130   \n",
       "3      2457   2749   4058   2000   3499   8817   1997  17800  24694   2094   \n",
       "4      8037   2360   8398  10217   2000   2147   2006   7521   3021   1010   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "9895  15536   3211  19738   5705  14456   2000  29082   2039  24256   2007   \n",
       "9896   8398  23363   2015   3951  10153   2006   7349   2708   5347   2899   \n",
       "9897   8398   9559   2360   3648  14087   7360   2005  27652   9870   2047   \n",
       "9898   3422   1024   2157   1011   3358   9220  23123   6495   8398   2005   \n",
       "9899   5977  17688   2099  26316   2135  11180   2004  21868   2232   1008   \n",
       "\n",
       "      ...  6330  6331  6332  6333  6334  6335  6336  6337  6338  New_Label  \n",
       "0     ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0          0  \n",
       "1     ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0          1  \n",
       "2     ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0          1  \n",
       "3     ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0          0  \n",
       "4     ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0          1  \n",
       "...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...        ...  \n",
       "9895  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0          0  \n",
       "9896  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0          1  \n",
       "9897  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0          1  \n",
       "9898  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0          0  \n",
       "9899  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0          0  \n",
       "\n",
       "[9900 rows x 6340 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = final_data.fillna(0)\n",
    "complete_work = final_data.drop(['Label'], axis = 1)\n",
    "complete_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = complete_work['New_Label'].values\n",
    "X = complete_work.drop(['New_Label'], axis = 1).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 20)                126800    \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 15)                315       \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 10)                160       \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 127,336\n",
      "Trainable params: 127,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 20\n",
    "hidden_nodes_layer2 = 15\n",
    "hidden_nodes_layer3 = 10\n",
    "hidden_nodes_layer4 = 5\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "# fourth hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6339"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.5837 - accuracy: 0.6855\n",
      "Epoch 2/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 0.5236 - accuracy: 0.7169\n",
      "Epoch 3/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 0.4770 - accuracy: 0.7684\n",
      "Epoch 4/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.4244 - accuracy: 0.8027\n",
      "Epoch 5/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.4189 - accuracy: 0.8264\n",
      "Epoch 6/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.3227 - accuracy: 0.8525\n",
      "Epoch 7/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.2758 - accuracy: 0.8772\n",
      "Epoch 8/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.2331 - accuracy: 0.8991\n",
      "Epoch 9/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.2046 - accuracy: 0.9160\n",
      "Epoch 10/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 0.1859 - accuracy: 0.9270\n",
      "Epoch 11/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.3548 - accuracy: 0.9356\n",
      "Epoch 12/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1431 - accuracy: 0.9473\n",
      "Epoch 13/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.1213 - accuracy: 0.9577\n",
      "Epoch 14/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0957 - accuracy: 0.9669\n",
      "Epoch 15/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.0797 - accuracy: 0.9712\n",
      "Epoch 16/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0684 - accuracy: 0.9768\n",
      "Epoch 17/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0904 - accuracy: 0.9682\n",
      "Epoch 18/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.1173 - accuracy: 0.9591\n",
      "Epoch 19/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.0574 - accuracy: 0.9818\n",
      "Epoch 20/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.1944 - accuracy: 0.9403\n",
      "Epoch 21/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.0540 - accuracy: 0.9820\n",
      "Epoch 22/200\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.0342 - accuracy: 0.9903\n",
      "Epoch 23/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.0273 - accuracy: 0.9934\n",
      "Epoch 24/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.0204 - accuracy: 0.9953\n",
      "Epoch 25/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.0176 - accuracy: 0.9956\n",
      "Epoch 26/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 0.0159 - accuracy: 0.9961\n",
      "Epoch 27/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0208 - accuracy: 0.9939\n",
      "Epoch 28/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0584 - accuracy: 0.9785\n",
      "Epoch 29/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0503 - accuracy: 0.9821\n",
      "Epoch 30/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.3251 - accuracy: 0.9487\n",
      "Epoch 31/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.0829 - accuracy: 0.9864\n",
      "Epoch 32/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0471 - accuracy: 0.9953\n",
      "Epoch 33/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0231 - accuracy: 0.9977\n",
      "Epoch 34/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0189 - accuracy: 0.9973\n",
      "Epoch 35/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0960 - accuracy: 0.9982\n",
      "Epoch 36/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0076 - accuracy: 0.9985\n",
      "Epoch 37/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.0102 - accuracy: 0.9978\n",
      "Epoch 38/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0072 - accuracy: 0.9987\n",
      "Epoch 39/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0064 - accuracy: 0.9987\n",
      "Epoch 40/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0080 - accuracy: 0.9978\n",
      "Epoch 41/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0115 - accuracy: 0.9972\n",
      "Epoch 42/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.2581 - accuracy: 0.9255\n",
      "Epoch 43/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0622 - accuracy: 0.9786\n",
      "Epoch 44/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0171 - accuracy: 0.9952\n",
      "Epoch 45/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0077 - accuracy: 0.9982\n",
      "Epoch 46/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 0.0057 - accuracy: 0.9989\n",
      "Epoch 47/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0031 - accuracy: 0.9996\n",
      "Epoch 48/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0025 - accuracy: 0.9996\n",
      "Epoch 49/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 50/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 51/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 52/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 53/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 54/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 8.6903e-04 - accuracy: 0.9999\n",
      "Epoch 55/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 7.8857e-04 - accuracy: 0.9999\n",
      "Epoch 56/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 6.7043e-04 - accuracy: 0.9999\n",
      "Epoch 57/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 6.0065e-04 - accuracy: 0.9999\n",
      "Epoch 58/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 5.7646e-04 - accuracy: 0.9999\n",
      "Epoch 59/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 5.0569e-04 - accuracy: 0.9999\n",
      "Epoch 60/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 4.6623e-04 - accuracy: 0.9999\n",
      "Epoch 61/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 4.3699e-04 - accuracy: 0.9999\n",
      "Epoch 62/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 4.1184e-04 - accuracy: 0.9999\n",
      "Epoch 63/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 3.9185e-04 - accuracy: 0.9999\n",
      "Epoch 64/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 3.7412e-04 - accuracy: 0.9999\n",
      "Epoch 65/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 3.6284e-04 - accuracy: 0.9999\n",
      "Epoch 66/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 3.4725e-04 - accuracy: 0.9999\n",
      "Epoch 67/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 3.3633e-04 - accuracy: 0.9999\n",
      "Epoch 68/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 3.2582e-04 - accuracy: 0.9999\n",
      "Epoch 69/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 0.0511 - accuracy: 0.9922\n",
      "Epoch 70/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.3260 - accuracy: 0.9095\n",
      "Epoch 71/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.1732 - accuracy: 0.9798\n",
      "Epoch 72/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.0164 - accuracy: 0.9966\n",
      "Epoch 73/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.0100 - accuracy: 0.9978\n",
      "Epoch 74/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0042 - accuracy: 0.9997\n",
      "Epoch 75/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0028 - accuracy: 0.9996\n",
      "Epoch 76/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 8.1774e-04 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 6.2339e-04 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 4.8560e-04 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 3.9392e-04 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 3.2491e-04 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 2.6957e-04 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 2.2480e-04 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.9083e-04 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 1.6377e-04 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.3873e-04 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.1938e-04 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.1898e-04 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 8.9846e-05 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 7.5917e-05 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 6.4945e-05 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 5.6355e-05 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 4.9027e-05 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 4.2472e-05 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 3.6764e-05 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 3.1923e-05 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 2.7683e-05 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 2.4346e-05 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 2.0784e-05 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 1.8149e-05 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.5753e-05 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.3580e-05 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.1840e-05 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 1.0327e-05 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 8.9554e-06 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 7.8383e-06 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 6.8619e-06 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 6.0355e-06 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 5.1960e-06 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 4.5266e-06 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 3.9155e-06 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 3.3684e-06 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 2.9963e-06 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 2.6186e-06 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 2.3069e-06 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 2.0087e-06 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 1.7650e-06 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 1.5235e-06 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 1.3740 - accuracy: 0.9434\n",
      "Epoch 120/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.2108 - accuracy: 0.9752\n",
      "Epoch 121/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0599 - accuracy: 0.9934\n",
      "Epoch 122/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0080 - accuracy: 0.9982\n",
      "Epoch 123/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.0034 - accuracy: 0.9997\n",
      "Epoch 124/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 9.9222e-04 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 7.8658e-04 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 7.4394e-04 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 5.3580e-04 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 4.2475e-04 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 3.3321e-04 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 2.5063e-04 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.9997e-04 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.6672e-04 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 1.4111e-04 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 1.2290e-04 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 1.1494e-04 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 9.3758e-05 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 8.2940e-05 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 7.4688e-05 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "233/233 [==============================] - 9s 41ms/step - loss: 6.4093e-05 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 5.6904e-05 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 5.0933e-05 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 4.5073e-05 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 4.0946e-05 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 3.5321e-05 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 3.1513e-05 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "233/233 [==============================] - 9s 40ms/step - loss: 2.8100e-05 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 2.4938e-05 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 0.0733 - accuracy: 0.9868\n",
      "Epoch 151/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 0.1539 - accuracy: 0.9490\n",
      "Epoch 152/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.0431 - accuracy: 0.9872\n",
      "Epoch 153/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.0122 - accuracy: 0.9968\n",
      "Epoch 154/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.0030 - accuracy: 0.9996\n",
      "Epoch 155/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 156/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 7.9902e-04 - accuracy: 0.9999\n",
      "Epoch 157/200\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 6.4260e-04 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 4.4882e-04 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 3.6589e-04 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 3.1056e-04 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 2.6514e-04 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 2.2660e-04 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 1.9468e-04 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 1.6762e-04 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 1.4672e-04 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 1.3689e-04 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 1.1026e-04 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 9.6456e-05 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "233/233 [==============================] - 11s 45ms/step - loss: 8.5156e-05 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 7.4893e-05 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 6.5252e-05 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 5.7294e-05 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 5.0499e-05 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 4.4652e-05 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 3.9581e-05 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 3.4042e-05 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 2.9513e-05 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 2.5858e-05 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "233/233 [==============================] - 11s 45ms/step - loss: 2.2803e-05 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 2.0144e-05 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.7643e-05 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.5505e-05 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 1.3608e-05 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.2035e-05 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.0444e-05 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 9.2922e-06 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "233/233 [==============================] - 11s 47ms/step - loss: 7.9741e-06 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 7.3309e-06 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 6.1630e-06 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 5.4584e-06 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 4.7511e-06 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "233/233 [==============================] - 10s 45ms/step - loss: 4.1753e-06 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "233/233 [==============================] - 11s 45ms/step - loss: 3.6685e-06 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "233/233 [==============================] - 10s 44ms/step - loss: 3.2118e-06 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 2.8364e-06 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 2.4682e-06 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 2.1964e-06 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 1.9114e-06 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 3.3737e-06 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "233/233 [==============================] - 10s 42ms/step - loss: 1.7935e-06 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 - 3s - loss: 5.0976 - accuracy: 0.6844 - 3s/epoch - 42ms/step\n",
      "Loss: 5.097559928894043, Accuracy: 0.6844444274902344\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model's file path\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(\"saved_models/fake_vs_real_5.h5\")\n",
    "\n",
    "\n",
    "# Export your model to a HDF5 file\n",
    "nn.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Set the model's file path\n",
    "file_path = Path(\"saved_models/fake_vs_real_5.h5\")\n",
    "\n",
    "\n",
    "# Load the model to a new object\n",
    "nn_imported = tf.keras.models.load_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 3s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing data\n",
    "predictions = (nn.predict(X_test_scaled)).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predictions  actual\n",
       "0            0       0\n",
       "1            1       1\n",
       "2            0       0\n",
       "3            1       1\n",
       "4            1       1\n",
       "5            1       0\n",
       "6            0       0\n",
       "7            0       0\n",
       "8            0       1\n",
       "9            0       0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to compare the predictions with the actual values\n",
    "results = pd.DataFrame({\"predictions\": predictions.ravel(), \"actual\": y_test})\n",
    "\n",
    "# Display sample data\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "sns.histplot(results['predictions'], ax=ax1)\n",
    "sns.histplot((results['actual']),ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 593,  603],\n",
       "       [ 162, 1117]], dtype=int64)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, predictions, labels = [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Real News       0.65      0.87      0.74      1279\n",
      "   Fake News       0.79      0.50      0.61      1196\n",
      "\n",
      "    accuracy                           0.69      2475\n",
      "   macro avg       0.72      0.68      0.68      2475\n",
      "weighted avg       0.72      0.69      0.68      2475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "target_names = [\"Real News\", \"Fake News\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
